\section{Performance Considerations}\label{performance_considerations}


\subsection{Transaction Throughput}

An important measurement of a blockchain system's performance is the number of
transaction bytes per-second ($\mathit{TBPS}$) it can sustain. Unlike the more
commonly used metric, transactions per-second, this number is not dependent on
the chosen size of a transaction (which would allow to manipulate it at will by
choosing different transactions sizes). Running an update mechanism on the
blockchain should not result in a substantial performance degradation.
Therefore, in this section we estimate the impact on performance that the
proposed update mechanism will have on a system's TBPS. Our estimations are
based on \emph{worst case scenarios}, which allow us to determine upper bounds
for the performance impact of the update mechanism.

Given a payload size in bytes ($\mathit{psize}$) that needs to be stored in a
blockchain, the blockchain's throughput measured in $\mathit{TBPS}$, and the
duration of the update process ($\mathit{duration}_u$), we can calculate the
percentage of the system's $\mathit{TBPS}$ ($\mathit{usage}_{\mathit{pct}}$) that
will be used by the update payload as follows:
$$\mathit{usage}_{\mathit{pct}} = 100\frac{\mathit{psize}}{\mathit{TBPS} ~
\mathit{duration}_u}$$

An update consists of several phases (ideation, implementation, approval, and
activation). In each phase, there are three types of messages being sent:
commits, reveals, and votes.

% Why do we consider only the voting period:
Before the voting phases, where votes can be cast by the participants, each
update requires only two messages spread across two stability windows, needed
for transactions to stabilize in the chain. These stability windows are quite
large, e.g. in Cardano the stability window is 1 day and a half. As a result,
only two messages need to be transmitted for the commit-reveal phase over a
large period of time, which means that a blockchain system can easily handle
this. This leave us with the voting phase as the sole source for performance
degradation that can be caused by the update mechanism.

% Why do we consider phases in isolation
In addition, note that we only need to consider the additional load introduced
by the update mechanism during a single phase. It is in the voting period of
each phase where the system should be able to handle the additional load, since
the update mechanism introduces very little load between voting phases.

We define the worst-case scenario for a voting period in terms of
\begin{itemize}
\item number of participants ($n_p$), e.g. voters (note that in the worst case
  scenario everybody will vote, regardless of their stake, which means that the
  stake distribution is irrelevant for this analysis)
\item number of update proposals being voted at the same time ($n_c$), during
  the same period (note that in the worst case scenario multiple update
  proposals will coincide in the start and end of the voting period, otherwise
  the system would have a larger time interval to distribute the load).
\item number of time a participant changes her vote ($n_c$), per-update proposal
\end{itemize}
Then, we can calculate the worst case scenario for the number of bytes that need
to be transmitted as part of the vote payload ($\mathit{psize}_v$) as:
$$\mathit{psize}_v = s_v \mathit{n_p} n_r n_c$$

The size in bytes for $s_v$ was obtained by calculating the size CBOR
encoding~\cite{RFC7049} of the vote payload of our prototype. This payload
includes:
\begin{itemize}
\item The hash of the voted SIP. We use 32 bytes hashes, so considering the 1
  byte CBOR tag this gives us a total 33 bytes.
\item The confidence (for, against, reject), which can be encoded in 1 byte
  (which also included the CBOR tag).
\item The key of the voter. We use 32 bytes keys, so this give us a total of 33
  bytes, when we consider the CBOR tag.
\item The vote signature. We consider 64 bytes signatures, which are accompanied
  by a 32 bytes key. This results in 64 + 32 + 1 bytes required for the
  signature.
\end{itemize}
So a vote requires in total 164 bytes.

Table~\ref{fig:tab:worst-case-analysis-voting-period} shows the results of the
worst case analysis for different parameter values, where $\mathit{duration}_v$
is the number of voting days, which was used to calculate the voting period
duration. For this analysis we use the $\mathit{TBPS}$ that Cardano currently
achieves in mainnet: $3.2$ Kb/s. This number is obtained from dividing the
maximum block body size, $64$ Kb, by the number of seconds per-block, $20$.

\begin{table}[htb]
	\centering
	% NOTE: DO NOT EDIT THE TABLE BELOW: It was generated by the
	% 'worst-case-analysis' benchmark, which can be found in the
	% `formal-spec/bench` directory.
  \begin{tabular}{| r | r | r | r | r |}
    \hline
    $n_p$ & $n_r$ & $n_c$ & $\mathit{duration_v}$ & $\mathit{usage_{pct}}$\\
    \hline
    1000 & 2 & 1 & 7 & 0.017\\
    1000 & 2 & 5 & 7 & 0.083\\
    1000 & 2 & 10 & 7 & 0.166\\
    10000 & 2 & 1 & 7 & 0.166\\
    10000 & 2 & 5 & 7 & 0.828\\
    10000 & 2 & 10 & 7 & 1.655\\
    100000 & 2 & 1 & 7 & 1.655\\
    100000 & 2 & 5 & 7 & 8.277\\
    100000 & 2 & 10 & 7 & 16.555\\
    1000000 & 2 & 1 & 7 & 16.555\\
    1000000 & 2 & 5 & 7 & 82.773\\
    1000000 & 2 & 10 & 7 & 165.546\\
    1000000 & 2 & 10 & 14 & 82.773\\
    1000000 & 2 & 10 & 30 & 38.627\\
    10000000 & 2 & 1 & 7 & 165.546\\
    10000000 & 2 & 5 & 7 & 827.729\\
    10000000 & 2 & 10 & 7 & 1655.458\\
    \hline
  \end{tabular}
	\caption[Worst-case analysis for voting period]{Worst-case analysis TBPS
	for a voting period}
	\label{fig:tab:worst-case-analysis-voting-period}
\end{table}

Figure~\ref{fig:usage-vs-participants} shows the worst case usage as a function
of the number of participants, assuming 10 concurrent update proposals, a 7 day
voting period, and each participant changing her vote twice.

\begin{figure}[htp]
	\centering
	% NOTE: DO NOT EDIT THE PICTURE BELOW: It was generated by the
	% 'worst-case-analysis' benchmark.
  \begin{tikzpicture}
    \begin{axis}[
      title={Participants vs usage percentage},
      xlabel={Participants},
      xmode=log,
      ylabel={Usage percentage},
      ymode=linear,
      ytick={0, 20, 50, 100, 150, 200},
      legend pos=north west,
      ymajorgrids=true,
      grid style=dashed,
      ]
      \addplot[color=black] table {sections/data/participants-vs-usage.dat};
    \end{axis}
  \end{tikzpicture}
	\caption{Worst case scenario analysis for system's usage with 10 concurrent
	update proposals}
	\label{fig:usage-vs-participants}
\end{figure}

Looking at Table~\ref{fig:tab:worst-case-analysis-voting-period} and
Figure~\ref{fig:usage-vs-participants}, we can see that the usage percentage
scales linearly in the number of participants\footnote{Note the logarithmic
  scale used in Figure~\ref{fig:usage-vs-participants}}, i.e., a 10 times
increase in the number of participants will increase 10 times the required usage
percent.
%
We can see that the impact of the update protocol on the
system's performance is negligible up to $100,000$ participants and $1$ proposal
being voted.

However, in spite of the usage percentage being a linear function of the number
of participants, when we pass the million of participants or $100,000$
participants vote on $10$ proposals at the same time, we start seeing a
considerable impact of the update protocol on the system's performance.
%
In case $1$ million participants would vote on $10$ proposals at the same time,
the system could not process the extra payload. Nevertheless, this would require
that the worst case conditions we assume in this analysis being met: $10$ SIP's
being voted at the same time over the period of $7$ days, where each participant
votes twice. In practice we will have a much higher voting duration, and not all
proposals will overlap exactly in their voting period, which means that the
additional payload can be spread across a much larger time interval.

There are several alternatives to allow a blockchain system to accommodate more
participants or concurrent update proposals:
\begin{itemize}
\item Use of expert pools. By having the participants delegate their voting
  rights, the number of voters can be substantially reduced, while increasing
  voter's turnout.
\item Increase in the duration of the voting period. We have used a very short
  voting duration (7 days). In practice update proposals would require that
  voters are given a much longer time to decide, specially if the update
  proposal has a large impact on the network.
\item Increase the maximum block size that the protocol allows, which will
  result in a larger TBPS. Note also that the current maximum block size in the
  Cardano protocol, which we use for our worst case analysis, is quite small but
  sufficient for accommodating the blockchain current usage. If the blockchain
  would need to process additional payload, experiments show that Cardano can
  increase its maximum block size up to $1$Mb, which would result in a tenfold
  increase in the TBPS, and therefore a tenfold increase in the number of
  participants that the update protocol can accommodate.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TODO: we need the revisit the section below considering the insight of
%% the section above.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Processing Time and Memory Consumption}
Clearly the most processing intesive task of the update mechanism
is the tally phase. It is the phase where all the collected votes
are counted in order to reach a decision for a specific proposal.

We start with a theoretical time complexity analysis where we
assume a worst-case scenario, where we have $n$ participants that all
of them vote by submiting a single vote. Also we assume that we have
a single proposal, so that within a voting period, the number $n$ of
participants coincides to the number of submitted ballots.

In the following we try to break up the operations during the tally phase. In
the heart of the tally phase lies the following function call, which is called
for each proposal.

\begin{lstlisting}[language=Haskell, caption=Tally phase initial function call]
	tallyStake confidence result ballot stakeDistribution adversarialStakeRatio
	=
	if stakeThreshold adversarialStakeRatio (totalStake stakeDistribution)
	<
	stakeOfKeys votingKeys stakeDistribution
	then Just result
	else Nothing
	where
	votingKeys = Map.filter (== confidence) ballot
\end{lstlisting}

Function \lstinline{stakeThreshold} is constant whereas \lstinline{Map.filter}
\footnote{\href{url}{http://hackage.haskell.org/package/containers-0.6.2.1/docs/Data-Map-Strict.html\#g:25}},
is $O(n)$ so \lstinline{votingKeys} is $O(n)$, where $n$ is the number of
ballots, which as we have said coincides to the number of participants. At this
point, we have a single pass (loop) over $n$ ballots.

The remaining function used in the definition of \lstinline{tallyStake},
\lstinline{stakeOfKeys}, calculates the stake associated to the given key-map,
and is defined as follows:
\begin{lstlisting}[language=Haskell, caption=Code example]
	stakeOfKeys
	keyMap
	StakeDistribution
	{ stakeMap
	}
	= Map.foldl (+) 0 (stakeMap `Map.intersection` keyMap)
\end{lstlisting}
The $intersection$ function in the worst-case is
$O(n)$\footnote{\href{url}{http://hackage.haskell.org/package/containers-0.6.2.1/docs/Data-Map-Strict.html\#v:intersection}}.
Therefore this is a second pass (loop) over the data of length $n$. Finally, we
call \lstinline{foldl} with a constant time operation, \lstinline{+}, which
means that this call is also
$O(n)$\footnote{\href{url}{http://hackage.haskell.org/package/containers-0.6.2.1/docs/Data-Map-Strict.html\#v:foldl}}.
This is a third pass (loop) over the data of length $n$. Thus from the above
analysis we see that we have for a single proposal a call of
\lstinline{tallyStake}, where in each such call we have three passes over the
data of length $n$. So in total for a single proposal we do $3$ passes over the
data of length $n$. That is $3n$ operations, which means that the tally time
complexity is $O(n)$.

This result is also confirmed by the experimental evaluation shown in
Figure~\ref{fig:eltime-vs-participants}, where we see that the processing time
increases linearly with the number of participants. In addition, we see that it
takes almost one tenth of a second to process the votes of $1$ million
participants. These results correspond to a single-core execution of the tally
algorithm on a i7 CPU laptop with 32GB of RAM.

\begin{figure}[htp]
	\centering

	\begin{tikzpicture}
		\begin{axis}[
			title={Processing time (sec) vs Number of participants},
			xlabel={Participants},
			xmin=100.0,
			xmax=1.0e7,
			xmode=log,
			xtick={10.0, 100.0, 1000.0, 10000.0, 100000.0, 1000000.0, 1.0e7},
			ylabel={Processing time (sec)},
			ymin=0.0000001,
			ymax=10,
			ymode=log,
			ytick={0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1,
			10},
			legend pos=north west,
			ymajorgrids=true,
			grid style=dashed,
			]
			\addplot[color=black] table
			{sections/data/participants-vs-elapsed_time.dat};
		\end{axis}
	\end{tikzpicture}

	\caption{Worst case scenario analysis for tally phase processing time}
	\label{fig:eltime-vs-participants}
\end{figure}

Finally, we present the measurements of the memory consumption
during the tally phase. Again as the graph in
Figure \ref{fig:memcons-vs-participants} shows, the memory
allocated scales linearly in the number of participants. Moreover, our
measurements show that the allocated memory essentially corresponds to the
space required for storing the $256$ bit hashes of the public keys of the
participants in a Haskell map
structure\footnote{\href{url}{http://hackage.haskell.org/package/containers-0.6.2.1/docs/Data-Map-Strict.html}}.

\begin{figure}[htp]
	\centering

	\begin{tikzpicture}
		\begin{axis}[
			title={Memory consumed (MBs) vs Number of participants},
			xlabel={Participants},
			xmin=100.0,
			xmax=1.0e7,
			xmode=log,
			xtick={10.0, 100.0, 1000.0, 10000.0, 100000.0, 1000000.0, 1.0e7},
			ylabel={Memory consumed (MBs)},
			ymin=1,
			ymax=10000,
			ymode=log,
			ytick={1, 10, 100, 1000, 10000},
			legend pos=north west,
			ymajorgrids=true,
			grid style=dashed,
			]
			\addplot[color=black] table
			{sections/data/participants-vs-memory_consumed.dat};
		\end{axis}
	\end{tikzpicture}

	\caption{Worst case scenario analysis for tally phase consumed memory}
	\label{fig:memcons-vs-participants}
\end{figure}
