\section{Measurements specification} \label{sec:measurements} In this section we
want to describe an experimental evaluation of our proposed update mechanism.
This experimental evaluation will help to verify to what degree our proposal
fulfills non-functional requirements, such as the ones described in section
\ref{sec:non-func-reqs}.

\subsection{What to measure} \label{sec:what-to-measure} Our experimental
evaluation will mainly focus on the following metrics:

\paragraph{Transaction throughput}
\emph{High-level goal:} We want to evaluate the impact of the update protocol to
the transaction throughput of the blockchain system, i.e., to the number of
transactions that manage to get into a block in the unit of time.

If $N_{Tx}$ is the number of transactions that fit in a block defined as
$N_{Tx} = \frac{Block Size}{Avg Transaction Size}$ and a new block is issued
every $T_B$ units of time, then we define as \emph{transaction throughput}
$Tx_{th}$ the ratio $Tx_{th} = \frac{N_{Tx}}{T_B}$ and we usually measure it in
\emph{transactions per sec (tps)}. We want to evaluate the impact of the number
of users $N_u$ that actively participate in the update mechanism to this metric.

\paragraph{Blockchain size}
\emph{High-level goal:} We want to evaluate the impact of storing update
transactions (i.e., transactions with an update payload) within a block, to the
number of common transactions that can be stored in the unit of storage.

Lets consider a blockchain system running a consensus protocol. At time point
$T_{start}$ we omit $k$ blocks from the end of the chain ($k$ is the security
parameter of the protocol) and mark the slot of the last block in the remaining
chain as $S_{start}$. We let the consensus protocol run for a fixed time window
of $T_w$ units of time until $T_{end}$ ($T_w = T{end} - T{start}$). Similarly,
we omit $k$ blocks from the end of the chain and mark the slot of the first
block in the remaining chain as $S_{end}$. We define the size of the chain
between slot $S_{start}$ and slot $S_{end}$ (included), as the \emph{blockchain
  size of time window} $T_w$ and note it as $BSize_{T_w}$. We divide the number
of transactions stored in the chain of size $BSize_{T_w}$ with this size and get
the \emph{number of transactions per unit of storage}, noted as $Tx_{s}$. We
want to evaluate the impact of the number of users $N_u$ that actively
participate in the update mechanism to this metric.

\paragraph{Update time to activation}
\emph{High-level goal:} we want to measure the total time to complete a software
update (i.e., the end-to-end elapsed time from submission-of-proposal to
activation) and evaluate how is this time impacted by the the number of active
users participating in the update protocol.

A software update (SU) starts its life with the submission into the blockchain
of a \emph{system improvement proposal} (SIP) and ends upon the activation of
the SU. We define the elapsed time that takes for a software update to activate
from start to finish, excluding the human delays (e.g., the time it takes for
the implementation of the software update) as the \emph{update time to
  activation} and note $T_{act}$. We want to evaluate the impact of the number
of users $N_u$ that actively participate in the update mechanism to this metric.

\paragraph{Processing time}
\emph{High-level goal:} we want to measure the computation load of the update
protocol and identify the most computation-heavy phases of the update protocol.

To this end, we will measure the processing time spent by the node, in each
\emph{distinct phase} of the decentralized software update \emph{lifecycle}, as
the number of active users $N_u$ participating in the update protocol increases.

\subsection{How to measure}
We consider two different approaches for conducting the measurements. The two
alternatives have to do with the integration (or not) with the Cardano
blockchain and the underlying consensus protocol. In the \emph{single node}
alternative, we have a single node implementation that does not need to run the
consensus protocol; it only runs the software updates protocol over a generated
set of update events. In the \emph{networked node} alternative, multiple nodes
run the consensus protocol, as well as the software updates protocol and
communicate with each other.

In the single node case, the blockchain is generated by the generator process
and not by the node itself. The node reads each generated block and runs the
update protocol, since the block contains also transactions with update payload
apart from common transactions. So the update events stored in each block,
trigger a corresponding action from the part of the node that has to do with the
update protocol.

In the networked node case, the generator generates only transactions and not
blocks, which then transmits to the network to the running nodes. The nodes
listen to the network for these transactions and based on the rules of the
consensus protocol create blocks, which are then transmitted again to the
network for the other nodes to receive. Every node builds a local blockchain
based on the consensus protocol. We assume that apart from common transactions,
also transactions with update payload are generated, which trigger a node to run
the update protocol (as in the single-node case), but in this case this is done
along with the execution of the consensus protocol.

\subsubsection{Single node simulation}
\paragraph{Justification for the single node simulation}
In a realistic networked implementation of the update protocol, each participant
node will maintain a local blockchain consisting of common, as well as, update
transactions. We argue that for the specific measurements at hand (see section
\ref{sec:what-to-measure}) all the entailed information that we need, in order
to measure them correctly, has been incorporated in the end-product, which is
the local blockchain maintained by each node. Indeed, the number of common
transactions that managed to get through in the unit time of time, as well as
the number of common transactions that managed to get stored in a block, in the
presence of transactions with update payload, has been fully captured in the
produced blockchain. Therefore, we really do not need to actually run the
consensus layer in order to measure our metrics, but we need the result of the
consensus layer and that of the execution of the update protocol. As long as the
produced blockchain realistically represents such as an execution, the single
node measurements should be equivalent to the networked ones. The only
difference between the two approaches is that in the networked case, in some
cases (depends on the actual network setup) it is easier to produce a more
realistic blockchain end-product.

\paragraph{Set up}
We assume a single generator process. This generator process will simulate $N_u$
users running the software update protocol thus generating update events (i.e.,
update transactions) and $M_u$ users running the consensus protocol generating
common transactions.

We assume that a single user generates update transactions with a specific
weight compared to common transactions (e.g., 1 update transaction for every
1000 common transactions). As we increase the number of users actively
participating in the update protocol the weight that corresponds to the
generation of update events also increases. So the ratio of update transactions
to common transaction increases e.g., $r_u = 1/1000, 2/1000, ...$

The generator will produce a fixed length blockchain consisting of blocks that
store either common transactions, or update transactions. This blockchain will
trigger the node to run for a specific time window, either processing common
transactions, or running the update protocol. As the above ratio increases, we
expect to see the trace including more and more update transactions and less
common transactions. This is expected also, to impact at some point the running
time of the node, since the node will have to do more work due to the software
updates, but only at specific periods of the update protocol (e.g., the tally
phase) where the computation requirements are more intense.

It is important in the produced blockchain to simulate a realistic analogy of
stored transactions per block. To this end, in our size calculations we use the
actual Cardano \emph{maximum block size} $B_{max}$. In order to achieve a
realistic transactions per block analogy, we consider the
$TxPerBlock = \frac{Maximum Block Size}{Average Transaction Size}$, similarly
based on the Cardano benchmarks. So for example, if $TxPerBlock = 800$ and
$B_{max} = 1MB$, then we assume an abstract storage cost equal to $800$ units
for a block and an abstract storage cost of $1$ unit for a common transaction.
Then, for a transaction with an update payload, we consider an abstract storage
cost equal to $\frac{Actual Size of Update Payload}{B_{max}}$. In this way, our
abstract storage sizes simulate a realistic block size and more importantly, a
realistic analogy of common transactions and update transactions fitting in a
block.

\paragraph{Transaction throughput}
The number of common transactions included in a generated trace (i.e., a
blockchain consisting of common transactions and update transactions) of a
specific fixed length and corresponding to a specific ratio $r_u$, divided by
the total processing time of the node, will be the logical equivalent to the
transaction throughput in this setup. To compute the number of common
transactions, we only need to scan the produced blockchain. Similarly, to
compute the total processing time we multiply the number of blocks produced, by
the average time to produce a block (based on a Cardano benchmark measurement).

As more update transactions are added to the blockchain, the number of common
transactions stored in this fixed length blockchain will
decrease, %and also the processing time of the node will increase%
so we expect that the transaction throughput will decrease. We want to evaluate
experimentally the rate of this decrease as the $r_u$ ratio scales up.

\paragraph{Blockchain size}
The number of common transactions included in a generated trace (i.e., a
blockchain consisting of common transactions and update transactions) of a
specific fixed length and corresponding to a specific ratio $r_u$, divided by
the size of this blockchain, will be the measured Blockchain Size metric in this
setup. The number of common transactions is calculated as in the previous
metric. In order to compute the size of the produced blockchain, we take the
number of produced blocks and multiply them by the maximum size of a block
(based on the Cardano implementation).

As more update transactions are added to the blockchain the number of common
transactions per unit of storage is expected to decrease. We want to evaluate
experimentally the rate of this decrease as the $r_u$ ratio scales up.

\paragraph{Update time to activation}
We generate a fixed trace (i.e., a blockchain consisting of common transactions
and update transactions) of a specific fixed length and corresponding to a
specific ratio $r_u$. We scan this trace and for each SU encountered we
calculate the total elapsed time. To do this, we will use a \emph{representative
  transaction processing time} (e.g., time to block a transaction
$\approx 20 secs$), which will come from the Cardano benchmarks. For each update
transaction of a specific software update encountered, we add this
representative transaction processing time and thus calculate a total elapsed
time for the software update to be activated. Of course, we also need to take
into account other time periods like the stability windows, the voting periods
etc. So we will also need a \emph{representative time for a slot to be
  generated}, since all these periods correspond to specific number of slots.
Once we calculate the elapsed time for each software update, then we calculate
the 80th-percentile as the representative \emph{update time to activation}. As
more update transactions are added to the blockchain, this metric is expected to
increase. We want to evaluate experimentally the rate of this increase as the
$r_u$ ratio scales up.

\paragraph{Processing time}
We generate a fixed trace (i.e., a blockchain consisting of common transactions
and update transactions) of a specific fixed length and corresponding to a
specific ratio $r_u$. This trace will trigger the execution of the update
protocol by the node. For each distinct phase of the software update lifecycle,
we measure the actual processing time of the node. We want to evaluate the
processing time per distinct phase in order to identify the most
computation-heavy phases of the update protocol. As more update transactions are
added to the blockchain, the processing time is expected to increase, as more
work is done by the node. We want to evaluate experimentally the rate of this
increase as the $r_u$ ratio scales up.

\subsubsection{Networked node simulation}
\paragraph{Set up}
\paragraph{Transaction throughput}
\paragraph{Blockchain size}
\paragraph{Update time to activation}
